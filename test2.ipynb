{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connessione a spotipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyOAuth\n",
    "import json\n",
    "import os \n",
    "\n",
    "# Configura le tue credenziali Spotify\n",
    "client_id = os.getenv('YOUR_CLIENT_ID')\n",
    "client_secret = os.getenv('YOUR_CLIENT_SECRET')\n",
    "redirect_uri = os.getenv('YOUR_REDIRECT_URI')\n",
    "\n",
    "scope = \"user-library-read playlist-read-private\"\n",
    "\n",
    "sp = spotipy.Spotify(auth_manager=SpotifyOAuth(client_id=client_id,\n",
    "                                               client_secret=client_secret,\n",
    "                                               redirect_uri=redirect_uri,\n",
    "                                               scope=scope))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_features_and_analysis(track_ids, audio_analysis=False):\n",
    "    # Maximum 50 track_ids for track info\n",
    "    track_infos = sp.tracks(track_ids)['tracks']\n",
    "    \n",
    "    # Maximum 100 track_ids for audio features\n",
    "    features = sp.audio_features(track_ids)\n",
    "    \n",
    "    track_data = {\n",
    "        'album': [track['album']['name'] for track in track_infos],\n",
    "        'artist': [track['artists'][0]['name'] for track in track_infos],\n",
    "        'duration_s': [track['duration_ms'] / 1000 for track in track_infos],\n",
    "        'name': [track['name'] for track in track_infos],\n",
    "        'popularity': [track['popularity'] for track in track_infos],\n",
    "        'id': [track['id'] for track in track_infos],\n",
    "        # audio features\n",
    "        'acousticness': [feature['acousticness'] for feature in features],\n",
    "        'danceability': [feature['danceability'] for feature in features],\n",
    "        'energy': [feature['energy'] for feature in features],\n",
    "        'instrumentalness': [feature['instrumentalness'] for feature in features],\n",
    "        'key': [feature['key'] for feature in features],\n",
    "        'liveness': [feature['liveness'] for feature in features],\n",
    "        'loudness': [feature['loudness'] for feature in features],\n",
    "        'mode': [feature['mode'] for feature in features],\n",
    "        'speechiness': [feature['speechiness'] for feature in features],\n",
    "        'valence': [feature['valence'] for feature in features],\n",
    "        'tempo': [feature['tempo'] for feature in features],\n",
    "        'time_signature': [feature['time_signature'] for feature in features],\n",
    "    }\n",
    "\n",
    "    if audio_analysis:\n",
    "        analyses = [sp.audio_analysis(track['id']) for track in track_infos]\n",
    "        track_data.update({\n",
    "            'segments': [len(analysis['segments']) for analysis in analyses],\n",
    "            'bars': [len(analysis['bars']) for analysis in analyses],\n",
    "            'beats': [len(analysis['beats']) for analysis in analyses],\n",
    "            'sections': [len(analysis['sections']) for analysis in analyses],\n",
    "            'tatums': [len(analysis['tatums']) for analysis in analyses]\n",
    "        })\n",
    "    \n",
    "    return track_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Set the batch size (max 50 for Spotify API)\n",
    "batch_size = 5\n",
    "\n",
    "# Get the total number of saved tracks\n",
    "total_tracks = sp.current_user_saved_tracks(limit=1)['total']\n",
    "print(f\"Total tracks: {total_tracks}\")\n",
    "\n",
    "total_tracks = 5\n",
    "# Loop through batches of saved tracks\n",
    "for offset in range(0, total_tracks, batch_size):\n",
    "    tracks = sp.current_user_saved_tracks(limit=batch_size, offset=offset)\n",
    "    track_ids = [item['track']['id'] for item in tracks['items']]\n",
    "    print(f\"Track IDs: {track_ids}\")    \n",
    "    # Apply the get_audio_features_and_analysis function\n",
    "    track_data = get_audio_features_and_analysis(track_ids)\n",
    "    # Append the result to the list\n",
    "    results.append(track_data)\n",
    "\n",
    "# Print or process the results as needed\n",
    "for result in results:\n",
    "    print(\"---\")\n",
    "    print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "def update_pickle(new_data, pickle_file='spotify_data.pickle'):\n",
    "    if os.path.exists(pickle_file):\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            existing_data = pickle.load(f)\n",
    "        for key in new_data:\n",
    "            if key in existing_data:\n",
    "                existing_data[key].extend(new_data[key])\n",
    "            else:\n",
    "                existing_data[key] = new_data[key]\n",
    "    else:\n",
    "        existing_data = new_data\n",
    "    \n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(existing_data, f)\n",
    "\n",
    "# Set the batch size (max 50 for Spotify API)\n",
    "batch_size = 5\n",
    "\n",
    "# Get the total number of saved tracks\n",
    "total_tracks = sp.current_user_saved_tracks(limit=1)['total']\n",
    "print(f\"Total tracks: {total_tracks}\")\n",
    "\n",
    "# Uncomment the next line if you want to limit the number of tracks for testing\n",
    "# total_tracks = 5\n",
    "\n",
    "# Loop through batches of saved tracks\n",
    "for offset in range(0, total_tracks, batch_size):\n",
    "    tracks = sp.current_user_saved_tracks(limit=batch_size, offset=offset)\n",
    "    track_ids = [item['track']['id'] for item in tracks['items']]\n",
    "    print(f\"Processing track IDs: {track_ids}\")    \n",
    "    \n",
    "    # Apply the get_audio_features_and_analysis function\n",
    "    track_data = get_audio_features_and_analysis(track_ids)\n",
    "    \n",
    "    # Update the pickle file with this batch of data\n",
    "    update_pickle(track_data)\n",
    "    \n",
    "    print(f\"Processed and saved tracks {offset} to {min(offset+batch_size, total_tracks)}\")\n",
    "    \n",
    "    # Optional: print the data for this batch\n",
    "    print(\"---\")\n",
    "    print(json.dumps(track_data, indent=4))\n",
    "\n",
    "print(\"All data processed and saved.\")\n",
    "\n",
    "# If you want to read and display all the data at the end:\n",
    "with open('spotify_data.pickle', 'rb') as f:\n",
    "    all_data = pickle.load(f)\n",
    "\n",
    "print(\"\\nAll data from pickle file:\")\n",
    "print(json.dumps(all_data, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m current_batch \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     15\u001b[0m batch_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresults\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Process the track and get audio features/analysis as before\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     track \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrack\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     20\u001b[0m     track_id \u001b[38;5;241m=\u001b[39m track[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a directory to store the JSON files if it doesn't exist\n",
    "if not os.path.exists('temp_data'):\n",
    "    os.makedirs('temp_data')\n",
    "\n",
    "# Save each batch of songs to a separate JSON file\n",
    "def save_batch(batch, batch_number):\n",
    "    filename = f'temp_data/songs_batch_{batch_number}.json'\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(batch, f)\n",
    "    print(f\"Saved batch {batch_number} to {filename}\")\n",
    "\n",
    "# In your main loop where you process songs:\n",
    "batch_size = 20\n",
    "current_batch = []\n",
    "batch_number = 1\n",
    "\n",
    "for item in results['items']:\n",
    "    # Process the track and get audio features/analysis as before\n",
    "    track = item['track']\n",
    "    track_id = track['id']\n",
    "    audio_info = get_audio_features_and_analysis(track_id)\n",
    "    \n",
    "    song_data = {\n",
    "        \"uri\": track['uri'],\n",
    "        \"name\": track['name'],\n",
    "        \"artist\": track['artists'][0]['name'],\n",
    "        \"album\": track['album']['name'],\n",
    "        \"popularity\": track['popularity'],\n",
    "        \"added_at\": item['added_at'],\n",
    "        **audio_info\n",
    "    }\n",
    "    \n",
    "    current_batch.append(song_data)\n",
    "    \n",
    "    if len(current_batch) == batch_size:\n",
    "        save_batch(current_batch, batch_number)\n",
    "        current_batch = []\n",
    "        batch_number += 1\n",
    "\n",
    "# Save any remaining songs in the last batch\n",
    "if current_batch:\n",
    "    save_batch(current_batch, batch_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 667db56d7fc5727dcd1c9a7d, topology_type: Unknown, servers: [<ServerDescription ('localhost', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "import os\n",
    "\n",
    "mongo_uri = os.getenv('YOUR_MONGO_URI')\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(mongo_uri, server_api=ServerApi('1'))\n",
    "db = client['portfolio-db']\n",
    "collection = db['songs']\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to upload a batch of songs\n",
    "def upload_batch(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        songs = json.load(f)\n",
    "    \n",
    "    result = collection.insert_many(songs)\n",
    "    print(f\"Uploaded {len(result.inserted_ids)} songs from {filename}\")\n",
    "\n",
    "# Upload all batches\n",
    "for filename in os.listdir('temp_data'):\n",
    "    if filename.endswith('.json'):\n",
    "        upload_batch(os.path.join('temp_data', filename))\n",
    "\n",
    "print(\"All data uploaded to MongoDB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le altre cose da importare oltre ai saved tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_liked_songs(sp):\n",
    "    results = sp.current_user_saved_tracks()\n",
    "    liked_songs = []\n",
    "    while results:\n",
    "        for item in results['items']:\n",
    "            track = item['track']\n",
    "            liked_songs.append(track)\n",
    "        if results['next']:\n",
    "            results = sp.next(results)\n",
    "        else:\n",
    "            results = None\n",
    "    return liked_songs\n",
    "\n",
    "def get_liked_albums(sp):\n",
    "    results = sp.current_user_saved_albums()\n",
    "    liked_albums = []\n",
    "    while results:\n",
    "        for item in results['items']:\n",
    "            album = item['album']\n",
    "            for track in album['tracks']['items']:\n",
    "                liked_albums.append(track)\n",
    "        if results['next']:\n",
    "            results = sp.next(results)\n",
    "        else:\n",
    "            results = None\n",
    "    return liked_albums\n",
    "\n",
    "def get_playlist_tracks(sp):\n",
    "    results = sp.current_user_playlists()\n",
    "    playlist_tracks = []\n",
    "    while results:\n",
    "        for playlist in results['items']:\n",
    "            tracks = sp.playlist_tracks(playlist['id'])\n",
    "            for item in tracks['items']:\n",
    "                track = item['track']\n",
    "                playlist_tracks.append(track)\n",
    "        if results['next']:\n",
    "            results = sp.next(results)\n",
    "        else:\n",
    "            results = None\n",
    "    return playlist_tracks\n",
    "\n",
    "liked_songs = get_liked_songs(sp)\n",
    "liked_albums = get_liked_albums(sp)\n",
    "playlist_tracks = get_playlist_tracks(sp)\n",
    "\n",
    "all_tracks = liked_songs + liked_albums + playlist_tracks\n",
    "\n",
    "# Rimuovi duplicati basati sull'ID della canzone\n",
    "unique_tracks = {track['id']: track for track in all_tracks}.values()\n",
    "\n",
    "# Stampa i titoli delle canzoni\n",
    "for track in unique_tracks:\n",
    "    print(track['name'], '-', track['artists'][0]['name'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postgres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import json\n",
    "from psycopg2.extras import Json\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"database\": \"your_database_name\",\n",
    "    \"user\": \"your_username\",\n",
    "    \"password\": \"your_password\"\n",
    "}\n",
    "\n",
    "# Assume 'results' is your list of JSON objects, one for each song\n",
    "\n",
    "# Create a table in your PostgreSQL database (run this once)\n",
    "def create_table():\n",
    "    conn = psycopg2.connect(**db_params)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS songs (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        name VARCHAR(255),\n",
    "        artist VARCHAR(255),\n",
    "        danceability FLOAT,\n",
    "        energy FLOAT,\n",
    "        key INTEGER,\n",
    "        loudness FLOAT,\n",
    "        mode INTEGER,\n",
    "        speechiness FLOAT,\n",
    "        acousticness FLOAT,\n",
    "        instrumentalness FLOAT,\n",
    "        liveness FLOAT,\n",
    "        valence FLOAT,\n",
    "        tempo FLOAT,\n",
    "        duration_ms INTEGER,\n",
    "        time_signature INTEGER,\n",
    "        segments INTEGER,\n",
    "        bars INTEGER,\n",
    "        beats INTEGER,\n",
    "        sections INTEGER,\n",
    "        tatums INTEGER,\n",
    "        full_data JSONB\n",
    "    )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "# Insert data into the database\n",
    "def insert_songs(songs):\n",
    "    conn = psycopg2.connect(**db_params)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    for song in songs:\n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO songs (\n",
    "            name, artist, danceability, energy, key, loudness, mode,\n",
    "            speechiness, acousticness, instrumentalness, liveness,\n",
    "            valence, tempo, duration_ms, time_signature, segments,\n",
    "            bars, beats, sections, tatums, full_data\n",
    "        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\", (\n",
    "            song['name'], song['artist'], song['danceability'], song['energy'],\n",
    "            song['key'], song['loudness'], song['mode'], song['speechiness'],\n",
    "            song['acousticness'], song['instrumentalness'], song['liveness'],\n",
    "            song['valence'], song['tempo'], song['duration_ms'], song['time_signature'],\n",
    "            song['segments'], song['bars'], song['beats'], song['sections'],\n",
    "            song['tatums'], Json(song)\n",
    "        ))\n",
    "\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the table (run this once)\n",
    "    create_table()\n",
    "\n",
    "    # Insert the songs\n",
    "    insert_songs(results)  # 'results' is your list of song JSONs\n",
    "\n",
    "    print(\"Data inserted successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "its",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
